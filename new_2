Great setup idea. With a big-RAM, no-GPU box running Ollama in Docker, here’s a production-ready playbook that squeezes the most throughput per dollar while keeping ops simple.

1) Model + quant choices (CPU-only)
	•	Prefer smaller/efficient quantizations (e.g., Q4_K_M for latency, Q5_K_M if you can spare RAM for a bit more quality). Keep one “general chat” model and a separate small reranker/embedding model for retrieval tasks to avoid loading a giant chat model for everything.
	•	Warm your main model on boot and keep it resident to avoid cold starts. You can do this via API keep_alive or an env var (details below). The API supports keep_alive and keeping models in RAM after requests.  ￼

2) Concurrency strategy
	•	Vertical concurrency: Let each container handle N parallel requests and M threads per request. Use:
	•	OLLAMA_NUM_PARALLEL to set how many requests run at once in one server process. (Default 1.)  ￼
	•	Threads per request: set via options in the request or with OLLAMA_NUM_THREADS (supported by Ollama/llama.cpp; behavior can vary by version—benchmark on your host). Rule of thumb: start near physical cores for threads per request, and set NUM_PARALLEL so total runnable threads ≈ 1–1.5× total cores. (Example on a 32-core CPU: NUM_PARALLEL=4, threads/request ~8.)
	•	Horizontal concurrency: Run multiple containers behind a reverse proxy. This is often more predictable than cranking NUM_PARALLEL very high in one process.

3) Memory mapping (mmap) vs full RAM
	•	Default is mmap (fast to “load”, uses OS pagecache, great for large files). For some filesystems / IO patterns, disabling mmap can help, but usually leave mmap ON unless you’ve profiled I/O stalls.
	•	If you do want to force “no mmap”, set OLLAMA_NO_MMAP=1 (global) or pass --no-mmap at runtime. (Environment variable added & discussed in the project.)  ￼
	•	Use keep_alive and a long value if you want the model to remain resident (see §4).  ￼

4) Kill cold starts with keep-alive
	•	Per request: {"keep_alive":"1h"} (or -1 to keep indefinitely).  ￼
	•	Server-wide default: OLLAMA_KEEP_ALIVE=1h (or longer) so UI/tools that don’t pass keep_alive still benefit.  ￼

5) Queueing & overload protection
	•	OLLAMA_MAX_QUEUE: increase if you expect bursts (default 512).
	•	OLLAMA_MAX_LOADED_MODELS: keep this low (1–2) so RAM isn’t fragmented across many models.
	•	OLLAMA_NUM_PARALLEL: tune for your CPU and latency targets. These are documented/used in the codebase and community guidance.  ￼

6) A solid Docker Compose (CPU-only, big-RAM)

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-cpu
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      # Networking
      OLLAMA_HOST: "0.0.0.0:11434"
      # Keep models hot
      OLLAMA_KEEP_ALIVE: "1h"
      # Concurrency knobs
      OLLAMA_NUM_PARALLEL: "4"        # start here; tune
      OLLAMA_MAX_QUEUE: "2048"        # burst tolerance
      OLLAMA_MAX_LOADED_MODELS: "1"   # avoid RAM thrash
      # Optional: disable mmap only if IO stalls / FS quirks
      # OLLAMA_NO_MMAP: "1"
      # Optional: threads per request (verify on your version)
      # OLLAMA_NUM_THREADS: "8"
    volumes:
      - ollama_models:/root/.ollama
    # Give it CPU and RAM it needs; pin if you run many replicas
    deploy:
      resources:
        limits:
          cpus: "32.0"     # example
          memory: "256g"   # example
volumes:
  ollama_models:

Notes
	•	Use a local SSD for /root/.ollama for faster paging when mmap is on.
	•	If you run several replicas, put NGINX/Traefik in front with simple least-connections and per-IP rate-limits.

7) Caching & warmup
	•	Preload on start to avoid the first user paying the tax:

curl -s http://localhost:11434/api/generate -d '{"model":"llama3.2:Q4_K_M","prompt":"","keep_alive":"1h"}' >/dev/null

(Empty prompt loads the model; keep_alive keeps it hot.)  ￼

	•	Check which models are loaded with GET /api/ps.  ￼

8) Request-level tuning from your service
	•	In your service’s POSTs to /api/chat or /api/generate, set:
	•	keep_alive (e.g., "30m"–"2h") for hot models.  ￼
	•	options: include num_ctx appropriate to your prompts (larger context increases RAM/latency), and (if supported in your version) num_thread to override threads per request for specific endpoints. (Exact option names vary by version; benchmark.)

9) Observability & SLOs
	•	Capture tokens/sec, queue wait time, latency percentiles per model. The API returns load/generation durations—log them.  ￼
	•	Alert on:
	•	queue depth ~ MAX_QUEUE
	•	ps output showing frequent unload/reload (tune keep-alive)  ￼
	•	OOM kills (lower MAX_LOADED_MODELS, smaller quant, or scale out)

10) Security & multi-tenant hygiene
	•	Bind to a private network and put reverse proxy auth (JWT/API key) in front.
	•	Per-tenant limits at the proxy (rate limit, request size).
	•	If multiple teams use different models, consider one replica group per model to avoid eviction churn.

11) When (and whether) to disable mmap
	•	Keep mmap enabled for most Linux + local-SSD deployments (fast “loads”, shared pagecache).
	•	Consider OLLAMA_NO_MMAP=1 only if you’ve measured page-fault churn or odd FS interactions; it will trade faster steady-state RAM access for slower loads.  ￼

⸻

Quick checklist to go live
	1.	Pick a quant (Q4_K_M first), warm it, set KEEP_ALIVE=1h.  ￼
	2.	Start with NUM_PARALLEL=2–4, threads/request ≈ physical cores / NUM_PARALLEL; load test and adjust.  ￼
	3.	Raise MAX_QUEUE if you have bursts; keep MAX_LOADED_MODELS=1 unless you truly need more.  ￼
	4.	Scale horizontally before pushing parallelism too high in one process.
	5.	Instrument tokens/sec + queue wait; pre-warm on deploy.  ￼

If you want, I can spit out a ready-to-run docker-compose.yml tailored to your server’s core count, RAM, and target QPS/latency—just tell me those numbers.