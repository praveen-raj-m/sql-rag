Got it — here’s a clean, PM-friendly catalog of every tag the script can assign, exactly how it’s calculated, and where Total Effect ( \text{Total} = \text{Allocation} + \text{Selection} ) fits in.

⸻

Core definition (used throughout)

\textbf{Total}_i \;=\; \text{Allocation}_i \;+\; \text{Selection}_i

This is the anchor for “moved-the-needle” tags, contribution share, sorting, and several global caps.

⸻

A) Material outcome tags (directly tied to Total)

These answer “who drove overall alpha this period?”

1) Top Contributor
	•	Screen: country is in the top percentile bucket by Total (default: top 10% of rows; if <10 rows, we still pick at least one).
	•	Global cap: keep 3 highest Total across all countries.
	•	Support stored: total, total_rank_percentile.
	•	Why Total matters: selection is by Total; ranking and the global cap also use Total.

2) Top Detractor
	•	Screen: country is in the bottom percentile bucket by Total (default: bottom 10%; min one when small N).
	•	Global cap: keep 3 most negative Total.
	•	Support: total, total_rank_percentile.
	•	Why Total matters: same as above, but on the downside.

⸻

B) Positioning vs outcome tags (Total used for tie-breaking)

These summarize the decision quadrant using excess weight and excess return; when too many qualify, we keep the ones with big impact (|Total|).

Let:
\text{Excess Wt} = W_p - W_b,\quad
\text{Excess Ret} = R_p - R_b

3) Overweight Outperformance (OW+beat)
	•	Screen: \text{Excess Wt} > 0 and \text{Excess Ret} > 0.
	•	Global cap: 3 max; if >3 qualify, keep the largest |Total|.
	•	Support: excess_weight, excess_return.

4) Underweight Underperformance (UW+lag)
	•	Screen: \text{Excess Wt} < 0 and \text{Excess Ret} < 0.
	•	Cap: 3 max; rank by |Total|.

5) Overweight Underperformance (OW+lag)
	•	Screen: \text{Excess Wt} > 0 and \text{Excess Ret} < 0.
	•	Cap: 3 max; rank by |Total|.

6) Underweight Outperformance (UW+beat)
	•	Screen: \text{Excess Wt} < 0 and \text{Excess Ret} > 0.
	•	Cap: 3 max; rank by |Total|.

Why Total matters here: these tags aren’t selected by Total, but when too many fit the quadrant, we keep the ones with the largest impact (|Total|) to stay concise.

⸻

C) Component strength/weakness tags (Total not used for selection, only for display/context)

These highlight where Allocation or Selection was unusually high/low relative to peers this period.

We compute cross-sectional percentiles:
\text{alloc\_pct}_i,\ \text{sel\_pct}_i
and z-scores for explanation in JSON:
z^\text{alloc}_i,\ z^\text{sel}_i

Defaults:
	•	Strong threshold = 84th percentile
	•	Weak threshold = 16th percentile

7) Strong Allocation
	•	Screen: \text{alloc\_pct} \ge strong_pct (default 84).
	•	Global cap: 3 max; tie-break on |Allocation|.
	•	Support: allocation, allocation_percentile, allocation_z.

8) Weak Allocation
	•	Screen: \text{alloc\_pct} \le weak_pct (default 16).
	•	Cap: 3 max; tie-break on |Allocation|.

9) Strong Selection
	•	Screen: \text{sel\_pct} \ge strong_pct.
	•	Cap: 3 max; tie-break on |Selection|.
	•	Support: selection, selection_percentile, selection_z.

10) Weak Selection
	•	Screen: \text{sel\_pct} \le weak_pct.
	•	Cap: 3 max; tie-break on |Selection|.

Where Total fits: not in the screen itself, but Total is what the PM sees first (and what our charts/ordering use). In prompts we still report Total alongside whichever component(s) are significant.

⸻

D) Efficiency & structure tags (Total only for secondary ranking)

11) High Efficiency Selection
	•	Metric: \text{selection\_efficiency} = \dfrac{\text{Selection}}{\max(W_p, 10^{-9})}.
	•	Screen: efficiency percentile ≥ efficiency_pct (default 90th).
	•	Global cap: 3 max; tie-break on Selection Efficiency magnitude.
	•	Support: selection_efficiency, portfolio_weight, selection, efficiency_percentile.

12) Divergent Effects
	•	Screen: Allocation and Selection have opposite signs:
\text{Allocation} \times \text{Selection} < 0
	•	Global cap: 3 max; when >3 qualify, rank by |Total| (bigger overall impact kept).
	•	Support: allocation, selection.

⸻

E) Caps & priorities (how we keep things readable)
	•	Per-record cap: each country keeps max 3 tags, using a priority order:
	1.	Top Contributor/Detractor
	2.	Quadrant tags (OW+beat, UW+lag, OW+lag, UW+beat)
	3.	Strong Selection / Strong Allocation
	4.	High Efficiency Selection
	5.	Divergent Effects
	6.	Weak Selection / Weak Allocation
	•	Global per-tag cap: each tag appears at most 3 times in the whole output.
	•	For Top Contributor/Detractor, the ranking metric is Total.
	•	For quadrant & divergent tags, if too many, we rank by |Total|.
	•	For strong/weak component tags, we rank by |component| (not Total).
	•	For efficiency, we rank by efficiency.

⸻

F) Significance in the prompt (how it’s written)

For the commentary prompt we only mention a component if it’s material:
	•	Show Allocation only if |\text{Allocation}| \ge 0.02 (default alloc_abs_min).
	•	Show Selection only if |\text{Selection}| \ge 0.02 (default sel_abs_min).
	•	Always show Total (rounded), plus Excess Wt/Ret when materially non-zero at 2dp.

This keeps the narrative centered on the largest Total drivers, with component detail only when it matters.

⸻

If you want any tag to rank differently (e.g., make Strong Selection pick its global top-3 by selection percentile instead of magnitude), say the word and I’ll switch that rule.


Right now, in the code we built, there’s no explicit numeric “weight” for tags — instead, we use a priority ranking system and global caps to decide:
	1.	Which tags survive if a country qualifies for many.
	2.	Which countries survive if too many qualify for the same tag.

That priority system is effectively a weighting of importance for a portfolio manager’s commentary.

⸻

A) Priority order we use now

This is the implicit “weight” — higher up means more important when there’s a clash.

Priority	Tag Group	Why it’s weighted high
1	Top Contributor / Top Detractor	These are the headline drivers — they directly answer “Who helped/hurt the most this period?” based on Total Effect.
2	Quadrant Tags (Overweight Outperformance, UW+lag, OW+lag, UW+beat)	These tell the story of positioning vs outcome — a PM wants to know if our active bets lined up with performance.
3	Strong Selection / Strong Allocation	Shows skill-based bets: selection skill (security picking) and allocation skill (weighting). High value to PMs as “decision quality” signals.
4	High Efficiency Selection	Measures how well a weight was “used” to generate excess return. Niche but useful — important for small but impactful weights.
5	Divergent Effects	Highlights conflicting drivers (e.g., good selection offset by bad allocation). Adds nuance to the commentary.
6	Weak Selection / Weak Allocation	Negative signals — important but not as headline-worthy unless they are top detractors. Often mentioned to explain underperformance.


⸻

B) Effective “weight” meaning
	•	This is not a mathematical weight in scoring.
	•	But when a country qualifies for more than 3 tags, we keep the top 3 based on this order.
	•	So in effect, Top Contributor/Detractor gets 100% survival rate if qualified, while Weak Allocation might get cut.

⸻

C) Why PMs value it in this order
	1.	Performance outcome first — In attribution, the first question is always “Who moved the needle?” (Total Effect).
	2.	Positioning logic — Did we win because we were right about the weight? Did we avoid something that hurt?
	3.	Skill signal — High selection/allocation effects speak to skill rather than luck.
	4.	Efficiency & nuance — Only after the above do we highlight small but efficient wins, or conflicting effects.
	5.	Weakness — Weakness is reported to explain underperformance but is not the primary story.

⸻

D) If you wanted actual numeric scores

We could convert the above priority into a “tag score” system for ranking inside each country, e.g.:

Tag Group	Score
Top Contributor / Detractor	100
Quadrant Tags	80
Strong Selection / Allocation	60
High Efficiency Selection	40
Divergent Effects	30
Weak Selection / Allocation	20

Then, inside a country:
	•	Sum the scores for all qualifying tags.
	•	Keep the tags with the highest scores (ties broken by magnitude of effect).

⸻

If you want, I can add a formal “weight score” system into the code, so it’s transparent in the JSON and you can later tweak it for different PM audiences.
That would also make RAG explanations better because we could say “This was kept because it’s a Top Contributor (score 100) and OW+beat (score 80)”.

Do you want me to build that scoring system in?