Perfect ‚Äî let's do everything **without `sentence-transformers` or any external model downloads**, and fully local using your **Ollama server** for generating **embeddings and chat**. This keeps your app lightweight and dependency-free.

---

## ‚úÖ Plan: Pure Ollama RAG (No Transformers, No Vector DB)

| Task                      | Tool                                       |
| ------------------------- | ------------------------------------------ |
| Get embeddings for chunks | ‚úÖ `POST /api/embeddings` (Ollama)          |
| Compare with user query   | ‚úÖ Cosine similarity (NumPy or manual math) |
| Chat with top-k chunks    | ‚úÖ `POST /api/chat` (Ollama)                |

---

## ‚úÖ Step-by-Step Backend Flow

---

### üîπ 1. Compute Embeddings on Upload (and store with each chunk)

#### üî∏ Update `store_chunks()` to include embeddings:

```python
import requests
import numpy as np

def get_ollama_embedding(text, model="nomic-embed-text"):
    res = requests.post(
        "http://localhost:11434/api/embeddings",
        json={"model": model, "prompt": text}
    )
    if res.status_code != 200:
        raise Exception("Embedding failed: " + res.text)
    return res.json()["embedding"]  # List[float]

def store_chunks(doc_name, chunks):
    file_path = os.path.join(CHUNK_DIR, "all_chunks.json")
    if os.path.exists(file_path):
        with open(file_path, "r") as f:
            all_chunks = json.load(f)
    else:
        all_chunks = []

    for c in chunks:
        c["source"] = doc_name
        c["embedding"] = get_ollama_embedding(c["text"])

    all_chunks.extend(chunks)

    with open(file_path, "w") as f:
        json.dump(all_chunks, f, indent=2)
```

---

### üîπ 2. During Chat: Get Query Embedding + Cosine Similarity

```python
def cosine_similarity(vec1, vec2):
    a = np.array(vec1)
    b = np.array(vec2)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

```python
def get_top_chunks(query, all_chunks, k=4):
    query_embedding = get_ollama_embedding(query)
    scored = []

    for chunk in all_chunks:
        sim = cosine_similarity(query_embedding, chunk["embedding"])
        scored.append((sim, chunk))

    scored.sort(reverse=True, key=lambda x: x[0])
    return [chunk for _, chunk in scored[:k]]
```

---

### üîπ 3. Chat Prompt Builder

```python
def build_chat_prompt(query, selected_chunks):
    context = "\n\n".join([f"[{c['source']} ‚Äì {c['ref']}]:\n{c['text']}" for c in selected_chunks])
    return f"""You are a legal assistant. Use the following document excerpts to answer the user's question.

User Question:
{query}

Relevant Context:
{context}

Answer clearly and cite the document name and line references.
"""
```

---

### üîπ 4. Final Chat Route (Flask)

```python
@chat_bp.route("/chat/legal-docs", methods=["POST"])
def chat_with_docs():
    data = request.json
    query = data["user_query"]
    model = data.get("model", "gemma:12b")
    temperature = float(data.get("temperature", 0.2))
    history = data.get("history", [])

    with open(os.path.join(CHUNK_DIR, "all_chunks.json")) as f:
        all_chunks = json.load(f)

    selected_chunks = get_top_chunks(query, all_chunks)
    prompt = build_chat_prompt(query, selected_chunks)

    messages = history + [{"role": "user", "content": prompt}]

    res = requests.post(
        "http://localhost:11434/api/chat",
        json={"model": model, "messages": messages, "options": {"temperature": temperature}}
    )

    if res.status_code != 200:
        return jsonify({"error": res.text}), 500

    return jsonify({"answer": res.json()["message"]["content"]})
```

---

## ‚úÖ Summary

| Step            | Tool                                                          |
| --------------- | ------------------------------------------------------------- |
| Embeddings      | Ollama `/api/embeddings` with `nomic-embed-text`              |
| Similarity      | `numpy` cosine similarity                                     |
| Chat            | Ollama `/api/chat`                                            |
| No Dependencies | ‚ùå No `sentence-transformers`, ‚ùå No `sklearn`, ‚ùå No vector DBs |

---

Ready to test it? I can help generate a sample upload + chat script or simplify this further into modules.
