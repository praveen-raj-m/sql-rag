Excellent. Here's your **complete backend route for summarizing a single legal document**, using **paragraph-aware chunking**, **traceable prompts**, and integration with a **local LLM via Ollama**.

---

## ‚úÖ Summary of What This Does:

* Accepts a POST request with:

  * `text` (full legal document)
  * `model` (e.g., `gemma:7b`)
  * `temperature` (e.g., `0.2`)
* Chunks the document by **paragraphs**, keeping context natural and intact
* Sends each chunk to the LLM using a detailed, traceable prompt
* Returns an aggregated, bullet-point summary with **line references**

---

## ‚úÖ Backend: `routes/summary.py`

```python
from flask import Blueprint, request, jsonify
from werkzeug.utils import secure_filename
import requests

summary_bp = Blueprint('summary', __name__)

def chunk_by_paragraphs(text, max_words=800):
    lines = text.splitlines()
    line_map = {i: line for i, line in enumerate(lines)}

    paragraphs = text.split("\n\n")
    chunks = []
    current_chunk = []
    current_word_count = 0
    current_start_line = 0
    current_line_count = 0
    line_index = 0

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        para_lines = para.splitlines()
        word_count = len(para.split())

        if current_word_count == 0:
            current_start_line = line_index

        current_chunk.append(para)
        current_word_count += word_count
        line_index += len(para_lines)

        if current_word_count >= max_words:
            chunk_text = "\n\n".join(current_chunk)
            ref = f"Lines {current_start_line}‚Äì{line_index}"
            chunks.append((ref, chunk_text))

            # Reset
            current_chunk = []
            current_word_count = 0

    if current_chunk:
        chunk_text = "\n\n".join(current_chunk)
        ref = f"Lines {current_start_line}‚Äì{line_index}"
        chunks.append((ref, chunk_text))

    return chunks

def build_prompt(ref, chunk):
    return f"""
You are a legal analyst reviewing a section of a legal agreement.

Your task is to extract all important legal insights, including obligations, rights, clauses, exceptions, payment terms, timelines, penalties, governing laws, and any subtle implications or risks.

Avoid general summaries. Focus on what a lawyer or compliance officer would want to know.
Include the reference: ‚Äú{ref}‚Äù in your bullet points.

Use concise bullet points.

Reference: {ref}

Document Section:
\"\"\"
{chunk}
\"\"\"
"""

def call_local_llm(model, temperature, prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",  # Change port if different
        json={
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature
            }
        }
    )
    if response.status_code != 200:
        raise Exception(f"LLM call failed: {response.text}")
    return response.json()["response"]

@summary_bp.route('/summarize', methods=['POST'])
def summarize_document():
    try:
        data = request.json
        full_text = data['text']
        model = data.get('model', 'gemma:7b')
        temperature = float(data.get('temperature', 0.2))

        chunks = chunk_by_paragraphs(full_text)
        all_summaries = []

        for ref, chunk in chunks:
            prompt = build_prompt(ref, chunk)
            result = call_local_llm(model, temperature, prompt)
            all_summaries.append(f"üìå **{ref}**\n{result.strip()}\n")

        return jsonify({
            "summary": "\n\n".join(all_summaries)
        })

    except Exception as e:
        return jsonify({"error": str(e)}), 500
```

---

## ‚úÖ Also Register This Blueprint in `app.py`

```python
from routes.summary import summary_bp
app.register_blueprint(summary_bp, url_prefix='/api/summary')
```

---

## ‚úÖ Test With a Sample JSON Payload

Send a POST to:

```
http://localhost:5000/api/summary/summarize
```

With body:

```json
{
  "text": "<insert full agreement here>",
  "model": "gemma:7b",
  "temperature": 0.2
}
```

---

## ‚úÖ Output Example

```json
{
  "summary": "üìå **Lines 0‚Äì75**\n- Agreement is between Party A and Party B...\n- Payment due quarterly by 15th...\n\nüìå **Lines 76‚Äì150**\n- Either party may terminate with 30-day notice...\n..."
}
```

---

## ‚úÖ What‚Äôs Next?

* Apply this same endpoint to **template document**
* Later: allow chat by storing chunks in memory or embeddings (RAG-style)
* Or generate a downloadable summary (PDF, Word)

Let me know if you want:

* JSON-formatted summary instead of Markdown
* Async/Ollama stream handling
* Live status per chunk (for progress bar)

Ready to move to template summarization next?
