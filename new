Perfect ‚Äî let‚Äôs implement your system in two parts:

---

## ‚úÖ PART 1: Upload and Chunking

This handles:

* Reading uploaded `.docx` files
* Chunking them (using paragraph-based logic)
* Appending chunks with `doc_name` as source
* Storing them in memory or file (we'll default to file-based JSON)

---

### üîπ 1.1: Dependencies

```bash
pip install python-docx
```

---

### üîπ 1.2: Chunking Function

```python
import docx
import json
import os

CHUNK_DIR = "doc_chunks"
os.makedirs(CHUNK_DIR, exist_ok=True)

def extract_text_from_docx(file_path):
    doc = docx.Document(file_path)
    return "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])

def chunk_text(text, max_words=800):
    paragraphs = text.split("\n\n")
    chunks = []
    current_chunk = []
    word_count = 0
    line_counter = 0

    for para in paragraphs:
        if not para.strip():
            continue

        word_count += len(para.split())
        current_chunk.append((line_counter, para))
        line_counter += para.count("\n") + 1

        if word_count >= max_words:
            combined = "\n\n".join(p for _, p in current_chunk)
            ref = f"Lines {current_chunk[0][0]}‚Äì{current_chunk[-1][0]}"
            chunks.append({"ref": ref, "text": combined})
            current_chunk = []
            word_count = 0

    if current_chunk:
        combined = "\n\n".join(p for _, p in current_chunk)
        ref = f"Lines {current_chunk[0][0]}‚Äì{current_chunk[-1][0]}"
        chunks.append({"ref": ref, "text": combined})

    return chunks

def store_chunks(doc_name, chunks):
    file_path = os.path.join(CHUNK_DIR, "all_chunks.json")
    if os.path.exists(file_path):
        with open(file_path, "r") as f:
            all_chunks = json.load(f)
    else:
        all_chunks = []

    for c in chunks:
        c["source"] = doc_name
    all_chunks.extend(chunks)

    with open(file_path, "w") as f:
        json.dump(all_chunks, f, indent=2)
```

---

### üîπ 1.3: Flask Upload Endpoint

```python
from flask import Blueprint, request, jsonify
from werkzeug.utils import secure_filename

upload_bp = Blueprint("upload", __name__)

@upload_bp.route("/upload-doc", methods=["POST"])
def upload_doc():
    file = request.files["file"]
    doc_name = request.form.get("doc_name") or secure_filename(file.filename)

    temp_path = os.path.join("uploads", secure_filename(file.filename))
    os.makedirs("uploads", exist_ok=True)
    file.save(temp_path)

    text = extract_text_from_docx(temp_path)
    chunks = chunk_text(text)
    store_chunks(doc_name, chunks)

    return jsonify({"message": f"{doc_name} uploaded and chunked."})
```

---

## ‚úÖ PART 2: Chat with Retrieved Chunks

You already have:

* Model, temperature
* History
* User question

Now you:

* Load all chunks from file
* Retrieve top-k relevant ones via cosine similarity
* Build a system prompt + history + user query
* Call `/api/chat` on Ollama

---

### üîπ 2.1: Setup for Embedding (In-Memory)

```python
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
import numpy as np

embed_model = SentenceTransformer("all-MiniLM-L6-v2")

def embed_chunks_and_query(chunks, query, top_k=4):
    chunk_texts = [c["text"] for c in chunks]
    chunk_embeddings = embed_model.encode(chunk_texts)
    query_embedding = embed_model.encode([query])

    sims = cosine_similarity(query_embedding, chunk_embeddings)[0]
    top_indices = np.argsort(sims)[::-1][:top_k]

    return [chunks[i] for i in top_indices]
```

---

### üîπ 2.2: Chat Prompt Builder

```python
def build_chat_prompt(query, selected_chunks):
    context = "\n\n".join([f"[{c['source']} ‚Äì {c['ref']}]:\n{c['text']}" for c in selected_chunks])
    return f"""You are a legal assistant. Based on the following document excerpts, answer the user's question.

User Question:
{query}

Relevant Context:
{context}

Answer clearly and cite the document name and line references in your response.
"""
```

---

### üîπ 2.3: Flask Chat Route

```python
import requests

@chat_bp.route("/chat/legal-docs", methods=["POST"])
def chat_with_docs():
    data = request.json
    query = data["user_query"]
    model = data.get("model", "gemma:12b")
    temperature = float(data.get("temperature", 0.2))
    history = data.get("history", [])

    with open(os.path.join(CHUNK_DIR, "all_chunks.json")) as f:
        all_chunks = json.load(f)

    selected_chunks = embed_chunks_and_query(all_chunks, query)
    prompt = build_chat_prompt(query, selected_chunks)

    messages = history + [{"role": "user", "content": prompt}]

    res = requests.post(
        "http://localhost:11434/api/chat",
        json={"model": model, "messages": messages, "options": {"temperature": temperature}}
    )

    if res.status_code != 200:
        return jsonify({"error": res.text}), 500

    return jsonify({"answer": res.json()["message"]["content"]})
```

---

## ‚úÖ Summary

| Functionality  | Handled By                                             |
| -------------- | ------------------------------------------------------ |
| Upload & chunk | `/upload-doc`                                          |
| Storage        | `all_chunks.json`                                      |
| Retrieval      | cosine similarity + top-k                              |
| Prompt build   | Context from matched chunks                            |
| Chat           | `/chat/legal-docs` endpoint calling Ollama `/api/chat` |

---

Would you like to:

* Structure stored chunks per user/session?
* Add search filters (e.g., only use ‚Äútemplate‚Äù chunks)?
* Persist embeddings for speed?

Let me know ‚Äî this is production-ready with minimal infra.



Perfect ‚Äî let's do everything **without `sentence-transformers` or any external model downloads**, and fully local using your **Ollama server** for generating **embeddings and chat**. This keeps your app lightweight and dependency-free.

---

## ‚úÖ Plan: Pure Ollama RAG (No Transformers, No Vector DB)

| Task                      | Tool                                       |
| ------------------------- | ------------------------------------------ |
| Get embeddings for chunks | ‚úÖ `POST /api/embeddings` (Ollama)          |
| Compare with user query   | ‚úÖ Cosine similarity (NumPy or manual math) |
| Chat with top-k chunks    | ‚úÖ `POST /api/chat` (Ollama)                |

---

## ‚úÖ Step-by-Step Backend Flow

---

### üîπ 1. Compute Embeddings on Upload (and store with each chunk)

#### üî∏ Update `store_chunks()` to include embeddings:

```python
import requests
import numpy as np

def get_ollama_embedding(text, model="nomic-embed-text"):
    res = requests.post(
        "http://localhost:11434/api/embeddings",
        json={"model": model, "prompt": text}
    )
    if res.status_code != 200:
        raise Exception("Embedding failed: " + res.text)
    return res.json()["embedding"]  # List[float]

def store_chunks(doc_name, chunks):
    file_path = os.path.join(CHUNK_DIR, "all_chunks.json")
    if os.path.exists(file_path):
        with open(file_path, "r") as f:
            all_chunks = json.load(f)
    else:
        all_chunks = []

    for c in chunks:
        c["source"] = doc_name
        c["embedding"] = get_ollama_embedding(c["text"])

    all_chunks.extend(chunks)

    with open(file_path, "w") as f:
        json.dump(all_chunks, f, indent=2)
```

---

### üîπ 2. During Chat: Get Query Embedding + Cosine Similarity

```python
def cosine_similarity(vec1, vec2):
    a = np.array(vec1)
    b = np.array(vec2)
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

```python
def get_top_chunks(query, all_chunks, k=4):
    query_embedding = get_ollama_embedding(query)
    scored = []

    for chunk in all_chunks:
        sim = cosine_similarity(query_embedding, chunk["embedding"])
        scored.append((sim, chunk))

    scored.sort(reverse=True, key=lambda x: x[0])
    return [chunk for _, chunk in scored[:k]]
```

---

### üîπ 3. Chat Prompt Builder

```python
def build_chat_prompt(query, selected_chunks):
    context = "\n\n".join([f"[{c['source']} ‚Äì {c['ref']}]:\n{c['text']}" for c in selected_chunks])
    return f"""You are a legal assistant. Use the following document excerpts to answer the user's question.

User Question:
{query}

Relevant Context:
{context}

Answer clearly and cite the document name and line references.
"""
```

---

### üîπ 4. Final Chat Route (Flask)

```python
@chat_bp.route("/chat/legal-docs", methods=["POST"])
def chat_with_docs():
    data = request.json
    query = data["user_query"]
    model = data.get("model", "gemma:12b")
    temperature = float(data.get("temperature", 0.2))
    history = data.get("history", [])

    with open(os.path.join(CHUNK_DIR, "all_chunks.json")) as f:
        all_chunks = json.load(f)

    selected_chunks = get_top_chunks(query, all_chunks)
    prompt = build_chat_prompt(query, selected_chunks)

    messages = history + [{"role": "user", "content": prompt}]

    res = requests.post(
        "http://localhost:11434/api/chat",
        json={"model": model, "messages": messages, "options": {"temperature": temperature}}
    )

    if res.status_code != 200:
        return jsonify({"error": res.text}), 500

    return jsonify({"answer": res.json()["message"]["content"]})
```

---

## ‚úÖ Summary

| Step            | Tool                                                          |
| --------------- | ------------------------------------------------------------- |
| Embeddings      | Ollama `/api/embeddings` with `nomic-embed-text`              |
| Similarity      | `numpy` cosine similarity                                     |
| Chat            | Ollama `/api/chat`                                            |
| No Dependencies | ‚ùå No `sentence-transformers`, ‚ùå No `sklearn`, ‚ùå No vector DBs |

---

Ready to test it? I can help generate a sample upload + chat script or simplify this further into modules.
