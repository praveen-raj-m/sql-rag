Great ‚Äî let‚Äôs implement the backend Flask `/chat` route in `attribution.py`.

---

### ‚úÖ Goal

Receive this from frontend:

```json
{
  "model": "llama3",
  "temperature": 0.7,
  "dataJson": { ... },         // from attribution prompt
  "history": [
    { "role": "user", "content": "What happened in energy?" },
    { "role": "assistant", "content": "Energy sector underperformed due to..." }
  ],
  "userQuery": "And healthcare?"
}
```

Return:

```json
{
  "response": "Healthcare sector contributed positively due to..."
}
```

---

### ‚úÖ Step-by-Step Code for `attribution.py`

```python
from flask import request, jsonify
from routes import attribution_bp
import requests

@attribution_bp.route('/chat', methods=['POST'])
def chat_with_llm():
    try:
        data = request.get_json()

        model = data.get('model', 'llama3')
        temperature = data.get('temperature', 0.7)
        attribution_json = data.get('dataJson', {})
        history = data.get('history', [])
        user_query = data.get('userQuery', '')

        if not user_query.strip():
            return jsonify({'response': 'No query provided.'}), 400

        # Build the prompt
        messages = [
            {
                "role": "system",
                "content": "You are a financial analyst. Answer user questions based on the attribution data provided."
            },
            {
                "role": "system",
                "content": f"Here is the attribution data:\n\n{attribution_json}"
            }
        ]

        # Add trimmed history
        for item in history[-6:]:
            messages.append({"role": item["role"], "content": item["content"]})

        # Add user query
        messages.append({"role": "user", "content": user_query})

        # Send to Ollama
        ollama_payload = {
            "model": model,
            "temperature": temperature,
            "messages": messages
        }

        ollama_response = requests.post(
            "http://localhost:11434/api/chat", json=ollama_payload, timeout=30
        )

        if ollama_response.status_code != 200:
            return jsonify({'response': '‚ö†Ô∏è LLM failed to respond'}), 500

        result = ollama_response.json()
        reply = result.get("message", {}).get("content", "").strip()

        return jsonify({'response': reply})

    except Exception as e:
        print("Error:", e)
        return jsonify({'response': '‚ö†Ô∏è Error during chat.'}), 500
```

---

### üß™ Test It

You can test with Postman or your Angular chat UI:

```bash
curl -X POST http://localhost:5000/chat \
-H "Content-Type: application/json" \
-d '{"model":"llama3", "temperature":0.7, "dataJson":{"Sector":"Energy",...}, "history":[], "userQuery":"What about energy?"}'
```

---

Ready to connect the frontend and try it out? Let me know if you want help improving the prompt quality or minimizing token use.
